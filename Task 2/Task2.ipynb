{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be46934-c600-4320-a064-a00bb997c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def load_pneumonia_data():\n",
    "    # Assuming X-ray images are loaded as `images` and labels `labels` (0: normal, 1: pneumonia)\n",
    "    images = np.array(...)  # Load X-ray images\n",
    "    labels = np.array(...)  # Load labels\n",
    "\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    images = images.astype('float32') / 255.0\n",
    "\n",
    "    # Resize to 224x224 for compatibility with InceptionV3\n",
    "    images = tf.image.resize(images, (224, 224))\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Step 2: Data Augmentation\n",
    "def augment_pneumonia_data(X_train):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "    return datagen.flow(X_train, batch_size=32)\n",
    "\n",
    "# Step 3: Model Development using Transfer Learning (InceptionV3)\n",
    "def build_pneumonia_model():\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False  # Freeze layers for transfer learning\n",
    "\n",
    "    # Add custom layers for binary classification\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Step 4: Training the model\n",
    "X_train, X_test, y_train, y_test = load_pneumonia_data()\n",
    "train_generator = augment_pneumonia_data(X_train)\n",
    "\n",
    "model = build_pneumonia_model()\n",
    "history = model.fit(train_generator, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 5: Evaluation\n",
    "# Plot training & validation accuracy and loss\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb8b752-a80b-435e-bc7c-0f07fbd060cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_indices = ~np.isnan(labels)  # Remove NaN labels\n",
    "images, labels = images[valid_indices], labels[valid_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31bd296-a79e-44bf-9827-e871f439c0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "labels = to_categorical(labels, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e46b9e-4019-48a9-ab5c-f466ba074788",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow(X_train, y_train, subset='training', batch_size=32)\n",
    "val_generator = datagen.flow(X_train, y_train, subset='validation', batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41370282-5f3c-4952-bb3e-ea01e8fe3237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d0fd0-818e-48ce-8b8f-f04178c8544c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e3aa59-5ffe-4be1-a67b-0839544a8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_generator, epochs=20, validation_data=val_generator, callbacks=[early_stopping, lr_scheduler])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d336e-97ce-4a7d-b24b-43dcbf8f44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-30]:  # Keep initial layers frozen\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile again with a lower learning rate\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_fine_tune = model.fit(train_generator, validation_data=val_generator, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc6846d-59ed-41e5-8c4f-b34fa6a045de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "history = model.fit(train_generator, epochs=10, validation_data=val_generator, class_weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61336082-23de-4482-b93e-b1137192337f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import seaborn as sns\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Pneumonia'], yticklabels=['Normal', 'Pneumonia'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "y_pred_prob = model.predict(X_test).ravel()\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b740140-c49c-40d7-a21a-bf5b2173f85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('pneumonia_detection_model.h5')\n",
    "\n",
    "# Reload for later use\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('pneumonia_detection_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e6d5ae-8d29-4336-9b2b-6d3074eb2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def grad_cam(model, img, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    conv_outputs = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    conv_outputs = tf.nn.relu(conv_outputs)\n",
    "\n",
    "    heatmap = tf.reduce_mean(conv_outputs, axis=-1)\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    return heatmap.numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
