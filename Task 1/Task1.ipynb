{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1c2a7e-6495-4957-9f94-fe3dac78c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 4us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m179s\u001b[0m 3us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 4us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import ResNet50, VGG16, InceptionV3\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load pre-trained models\n",
    "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "vgg_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "inception_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Custom layers to add on top of the base models\n",
    "def add_custom_layers(base_model):\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(10, activation='softmax')(x)  # Adjust for the number of classes\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    return model\n",
    "\n",
    "# Adding custom layers\n",
    "resnet_model = add_custom_layers(resnet_model)\n",
    "vgg_model = add_custom_layers(vgg_model)\n",
    "inception_model = add_custom_layers(inception_model)\n",
    "\n",
    "# Compile models\n",
    "resnet_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "vgg_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "inception_model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f54974-af35-4289-92da-789d2bf9bbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training for one model (use the other models in the same way)\n",
    "resnet_model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc0e6a-d19a-4ffd-9326-f7c739347b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make predictions for each model\n",
    "predictions_resnet = resnet_model.predict(test_data)\n",
    "predictions_vgg = vgg_model.predict(test_data)\n",
    "predictions_inception = inception_model.predict(test_data)\n",
    "\n",
    "# Average the predictions (for regression or probability values)\n",
    "final_predictions = (predictions_resnet + predictions_vgg + predictions_inception) / 3\n",
    "\n",
    "# For classification, you can choose the class with the highest probability\n",
    "final_predictions = np.argmax(final_predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930492f1-daf7-4027-8b64-1489b8165880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "def load_pneumonia_data():\n",
    "    # Assuming X-ray images are loaded as `images` and labels `labels` (0: normal, 1: pneumonia)\n",
    "    images = np.array(...)  # Load X-ray images\n",
    "    labels = np.array(...)  # Load labels\n",
    "\n",
    "    # Handle missing data\n",
    "    valid_indices = ~np.isnan(labels)\n",
    "    images, labels = images[valid_indices], labels[valid_indices]\n",
    "\n",
    "    # Normalize pixel values to [0, 1]\n",
    "    images = images.astype('float32') / 255.0\n",
    "\n",
    "    # Resize to 224x224 for compatibility with InceptionV3\n",
    "    images = tf.image.resize(images, (224, 224))\n",
    "\n",
    "    # Split the data into training, validation, and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Step 2: Data Augmentation\n",
    "def augment_pneumonia_data(X_train, y_train):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=30,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "\n",
    "    train_generator = datagen.flow(X_train, y_train, subset='training', batch_size=32)\n",
    "    val_generator = datagen.flow(X_train, y_train, subset='validation', batch_size=32)\n",
    "\n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Step 3: Model Development using Transfer Learning (InceptionV3)\n",
    "def build_pneumonia_model():\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False  # Freeze layers for transfer learning\n",
    "\n",
    "    # Add custom layers for binary classification\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=x)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Step 4: Training the Model\n",
    "X_train, X_test, y_train, y_test = load_pneumonia_data()\n",
    "train_generator, val_generator = augment_pneumonia_data(X_train, y_train)\n",
    "\n",
    "model = build_pneumonia_model()\n",
    "\n",
    "# Add Callbacks\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n",
    "]\n",
    "\n",
    "history = model.fit(train_generator, epochs=20, validation_data=val_generator, callbacks=callbacks)\n",
    "\n",
    "# Fine-tuning the Model\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-30]:  # Keep initial layers frozen\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_fine_tune = model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "# Step 5: Evaluation\n",
    "# Plot training & validation accuracy and loss\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='val loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Pneumonia'], yticklabels=['Normal', 'Pneumonia'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# ROC Curve\n",
    "y_pred_prob = model.predict(X_test).ravel()\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the Model\n",
    "model.save('pneumonia_detection_model.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
